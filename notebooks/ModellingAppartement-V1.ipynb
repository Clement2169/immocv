{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6963701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import main packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# define variables\n",
    "immo_vis_dir = \"../../data/immo_vis/\"\n",
    "parquet_extension = \".parquet\"\n",
    "metropole_appartement_file = \"ventes-metropole-appartement\" + parquet_extension\n",
    "metropole_maison_file = \"ventes-metropole-maison\" + parquet_extension\n",
    "metropole_appartement_file_cleaned = metropole_appartement_file + \"_step1_clean_\" + parquet_extension\n",
    "\n",
    "#  Set plot options\n",
    "LargePlotActive = False\n",
    "MediumPlotActive = False\n",
    "SmallPlotActive = False\n",
    "HeatMapDisplay = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0350b",
   "metadata": {},
   "source": [
    "# # functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cb43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions  \n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import time\n",
    "\n",
    "def print_numerical_isna(df) :\n",
    "    columns = df.select_dtypes(include='number').columns.tolist()\n",
    "    print (df[columns].isna().sum())\n",
    "\n",
    "def print_categorial_isna(df) :\n",
    "    columns = df.select_dtypes(include='category').columns.tolist()\n",
    "    print (df[columns].isna().sum())     \n",
    "\n",
    "def load_appartement_file (filename) :\n",
    "    start_path = Path(immo_vis_dir)\n",
    "    final_path = start_path / filename\n",
    "    return pd.read_parquet(final_path.as_posix())\n",
    "\n",
    "def save_appartement_file (df, filename) :\n",
    "    start_path = Path(immo_vis_dir)\n",
    "    final_path = start_path / filename\n",
    "    df.to_parquet(path=final_path.as_posix(),index=True)\n",
    "\n",
    "def get_numerical_column_names (df) :\n",
    "    return [ column for column  in df.columns if df[column].dtype not in [\"object\",\"category\"]]\n",
    "\n",
    "def apply_scale_processing(operator,X_train,X_test):\n",
    "    #Operator could be scaler or encode\n",
    "    op=operator\n",
    "    op.fit(X_train)\n",
    "    X_train_treated=pd.DataFrame(op.transform(X_train),columns=X_train.columns)\n",
    "    X_test_treated=pd.DataFrame(op.transform(X_test),columns=X_train.columns)\n",
    "    return X_train_treated,X_test_treated\n",
    "\n",
    "def apply_preprocessing  (df) :\n",
    "    y_tmp =df['prix_bien']\n",
    "    X_tmp =df.drop(columns=['prix_m2_vente','prix_bien','mapCoordonneesLatitude','mapCoordonneesLongitude','date'])\n",
    "    return X_tmp,y_tmp\n",
    "\n",
    "def create_train_test_data (X ,y) :\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #  build the train and test data\n",
    "    print (X.shape)\n",
    "    return train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "def create_train_test_data_subset (X ,y,subset_size = 1.0) :\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #  build the train and test data\n",
    "    print (X.shape)\n",
    "    return train_test_split(X,y,test_size=0.2,train_size=subset_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5beae9",
   "metadata": {},
   "source": [
    "# # decode file and display infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70b8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import appartement file\n",
    "\n",
    "df = load_appartement_file(metropole_appartement_file_cleaned)\n",
    "nb_rows= df.shape[0]\n",
    "nb_cols= df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b283a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ventes-metropole-appartement.parquet_step1_clean_.parquet rows 2077476 columns 47\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2077476 entries, immo-facile-57743459 to 134327817\n",
      "Data columns (total 47 columns):\n",
      " #   Column                   Dtype         \n",
      "---  ------                   -----         \n",
      " 0   etage                    int64         \n",
      " 1   surface                  int64         \n",
      " 2   nb_pieces                int64         \n",
      " 3   prix_bien                int64         \n",
      " 4   balcon                   int64         \n",
      " 5   eau                      int64         \n",
      " 6   bain                     int64         \n",
      " 7   mapCoordonneesLatitude   float64       \n",
      " 8   mapCoordonneesLongitude  float64       \n",
      " 9   annonce_exclusive        int64         \n",
      " 10  nb_etages                float64       \n",
      " 11  places_parking           float64       \n",
      " 12  cave                     int64         \n",
      " 13  annee_construction       float64       \n",
      " 14  nb_toilettes             int64         \n",
      " 15  nb_terraces              float64       \n",
      " 16  videophone               int64         \n",
      " 17  porte_digicode           int64         \n",
      " 18  ascenseur                int64         \n",
      " 19  nb_logements_copro       float64       \n",
      " 20  charges_copro            float64       \n",
      " 21  logement_neuf            int64         \n",
      " 22  date                     datetime64[ns]\n",
      " 23  CODE_IRIS                int64         \n",
      " 24  REG                      int64         \n",
      " 25  DEP                      int64         \n",
      " 26  prix_m2_vente            float64       \n",
      " 27  dpeL_num                 int64         \n",
      " 28  ges_class_num            int64         \n",
      " 29  chauf_energy_gaz         int64         \n",
      " 30  chauf_energy_elec        int64         \n",
      " 31  chauf_energy_bois        int64         \n",
      " 32  chauf_energy_fioul       int64         \n",
      " 33  chauf_sys_climatisation  int64         \n",
      " 34  chauf_sys_pompe_chaleur  int64         \n",
      " 35  chauf_sys_convecteur     int64         \n",
      " 36  chauf_sys_radiateur      int64         \n",
      " 37  chauf_sys_chaudiere      int64         \n",
      " 38  chauf_sys_sol            int64         \n",
      " 39  chauf_sys_poele_bois     int64         \n",
      " 40  chauf_mode_individuel    int64         \n",
      " 41  chauf_mode_collectif     int64         \n",
      " 42  chauf_mode_central       int64         \n",
      " 43  expo_has_nord            int64         \n",
      " 44  expo_has_sud             int64         \n",
      " 45  expo_has_est             int64         \n",
      " 46  expo_has_ouest           int64         \n",
      "dtypes: datetime64[ns](1), float64(9), int64(37)\n",
      "memory usage: 760.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#  print main infos on appartmeent file\n",
    "\n",
    "print ( f\" {metropole_appartement_file_cleaned} rows {nb_rows} columns {nb_cols}\")\n",
    "print (df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f8f453",
   "metadata": {},
   "source": [
    "# 1st Model Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6a7869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "etage                               int64\n",
       "surface                             int64\n",
       "nb_pieces                           int64\n",
       "prix_bien                           int64\n",
       "balcon                              int64\n",
       "eau                                 int64\n",
       "bain                                int64\n",
       "mapCoordonneesLatitude            float64\n",
       "mapCoordonneesLongitude           float64\n",
       "annonce_exclusive                   int64\n",
       "nb_etages                         float64\n",
       "places_parking                    float64\n",
       "cave                                int64\n",
       "annee_construction                float64\n",
       "nb_toilettes                        int64\n",
       "nb_terraces                       float64\n",
       "videophone                          int64\n",
       "porte_digicode                      int64\n",
       "ascenseur                           int64\n",
       "nb_logements_copro                float64\n",
       "charges_copro                     float64\n",
       "logement_neuf                       int64\n",
       "date                       datetime64[ns]\n",
       "CODE_IRIS                           int64\n",
       "REG                                 int64\n",
       "DEP                                 int64\n",
       "prix_m2_vente                     float64\n",
       "dpeL_num                            int64\n",
       "ges_class_num                       int64\n",
       "chauf_energy_gaz                    int64\n",
       "chauf_energy_elec                   int64\n",
       "chauf_energy_bois                   int64\n",
       "chauf_energy_fioul                  int64\n",
       "chauf_sys_climatisation             int64\n",
       "chauf_sys_pompe_chaleur             int64\n",
       "chauf_sys_convecteur                int64\n",
       "chauf_sys_radiateur                 int64\n",
       "chauf_sys_chaudiere                 int64\n",
       "chauf_sys_sol                       int64\n",
       "chauf_sys_poele_bois                int64\n",
       "chauf_mode_individuel               int64\n",
       "chauf_mode_collectif                int64\n",
       "chauf_mode_central                  int64\n",
       "expo_has_nord                       int64\n",
       "expo_has_sud                        int64\n",
       "expo_has_est                        int64\n",
       "expo_has_ouest                      int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porte_digicode [0 1]\n",
      "ascenceur [0 1]\n",
      "cave [0 1]\n"
     ]
    }
   ],
   "source": [
    "# check dtypes\n",
    "display(df.dtypes)\n",
    "print(\"porte_digicode\",df.porte_digicode.unique())\n",
    "print(\"ascenceur\",df.ascenseur.unique())\n",
    "print(\"cave\",df.ascenseur.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4d7a8",
   "metadata": {},
   "source": [
    "# # Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f91e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['etage', 'surface', 'nb_pieces', 'prix_bien', 'balcon', 'eau', 'bain',\n",
      "       'mapCoordonneesLatitude', 'mapCoordonneesLongitude',\n",
      "       'annonce_exclusive', 'nb_etages', 'places_parking', 'cave',\n",
      "       'annee_construction', 'nb_toilettes', 'nb_terraces', 'videophone',\n",
      "       'porte_digicode', 'ascenseur', 'nb_logements_copro', 'charges_copro',\n",
      "       'logement_neuf', 'date', 'CODE_IRIS', 'REG', 'DEP', 'prix_m2_vente',\n",
      "       'dpeL_num', 'ges_class_num', 'chauf_energy_gaz', 'chauf_energy_elec',\n",
      "       'chauf_energy_bois', 'chauf_energy_fioul', 'chauf_sys_climatisation',\n",
      "       'chauf_sys_pompe_chaleur', 'chauf_sys_convecteur',\n",
      "       'chauf_sys_radiateur', 'chauf_sys_chaudiere', 'chauf_sys_sol',\n",
      "       'chauf_sys_poele_bois', 'chauf_mode_individuel', 'chauf_mode_collectif',\n",
      "       'chauf_mode_central', 'expo_has_nord', 'expo_has_sud', 'expo_has_est',\n",
      "       'expo_has_ouest'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# set target and data\n",
    "print (df.columns)\n",
    "X,y = apply_preprocessing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b90f92",
   "metadata": {},
   "source": [
    "#  # create train test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c6f317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2077476, 42)\n"
     ]
    }
   ],
   "source": [
    "#  train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#  build the train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_train_test_data(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f8ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train_scaled,X_test_scaled=apply_scale_processing(MinMaxScaler(),X_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147007b",
   "metadata": {},
   "source": [
    "# # train ans predict models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad021695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_train_models (models,X_train, y_train,X_test,y_test) :\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        print(f\"Entraînement de {name}...\")\n",
    "        \n",
    "        # Mesurer le temps d'entraînement\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        fit_time = time.time() - start_time\n",
    "        \n",
    "        # Mesurer le temps de prédiction\n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        predict_time = time.time() - start_time\n",
    "        \n",
    "        # Calculer les métriques\n",
    "        rmse = root_mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Fit_Time': f\"{fit_time:.4f}s\",\n",
    "            'Predict_Time': f\"{predict_time:.4f}s\",\n",
    "            'Total_Time': f\"{fit_time + predict_time:.4f}s\"\n",
    "        })\n",
    "\n",
    "    # Afficher les résultats\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe46d597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement de LinearRegression...\n",
      "Entraînement de Ridge...\n",
      "Entraînement de ElasticNet...\n",
      "Entraînement de ElasticNetCV...\n",
      "Entraînement de LinearSVR...\n",
      "Entraînement de lgb...\n",
      "[LightGBM] [Info] Total Bins 1372\n",
      "[LightGBM] [Info] Number of data points in the train set: 1661980, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 305724.749736\n",
      "              Model           RMSE        R²  Fit_Time Predict_Time Total_Time\n",
      "5               lgb  127904.592827  0.790318   3.6342s      0.2355s    3.8697s\n",
      "0  LinearRegression  205814.163764  0.457077   3.6157s      0.0534s    3.6690s\n",
      "1             Ridge  206035.274962  0.455909   0.4553s      0.0226s    0.4778s\n",
      "4         LinearSVR  262389.232828  0.117571   3.8611s      0.0350s    3.8961s\n",
      "2        ElasticNet  262596.119190  0.116178   3.4031s      0.0159s    3.4189s\n",
      "3      ElasticNetCV  278863.585536  0.003284  13.7551s      0.0500s   13.8051s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression, Ridge\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "\n",
    "# Liste des modèles à tester\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "    'ElasticNetCV': ElasticNetCV(),\n",
    "    'LinearSVR': LinearSVR(),\n",
    "    # 'SVR': SVR(),\n",
    "    # 'Random Forest': RandomForestRegressor(n_estimators=20, random_state=42),\n",
    "    'lgb' : lgb.LGBMRegressor(\n",
    "                            objective='regression',\n",
    "                            metric='rmse',\n",
    "                            num_leaves=31,\n",
    "                            learning_rate=0.05,\n",
    "                            force_row_wise=True,\n",
    "                            # feature_fraction=0.9,\n",
    "                        )\n",
    "}\n",
    "\n",
    "# Tester chaque modèle avec mesure du temps\n",
    "results = fit_train_models(models,X_train_scaled,y_train,X_test_scaled,y_test)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df047c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement de XGBRegressor...\n",
      "Entraînement de DecisionTreeRegressor...\n",
      "                   Model           RMSE        R²  Fit_Time Predict_Time  \\\n",
      "0           XGBRegressor  112099.476562  0.838937  32.9283s      0.2059s   \n",
      "1  DecisionTreeRegressor  131380.640179  0.778767   9.7468s      0.1335s   \n",
      "\n",
      "  Total_Time  \n",
      "0   33.1342s  \n",
      "1    9.8802s  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Liste des modèles à tester\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "models = {\n",
    "    # 'KNeighborsRegressor': KNeighborsRegressor(n_neighbors=5),\n",
    "    'XGBRegressor' : XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "    # 'GradientBoostingRegressor': GradientBoostingRegressor(n_estimators=20, random_state=42),\n",
    "    # 'MLPRegressor': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),\n",
    "}\n",
    "\n",
    "# Tester chaque modèle avec mesure du temps\n",
    "\n",
    "results = fit_train_models(models,X_train,y_train,X_test,y_test)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6bf76",
   "metadata": {},
   "source": [
    "#  # improve hyper parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98730a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2077476, 42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X1,y1 = apply_preprocessing(df)\n",
    "X_train1, X_test1, y_train1,y_test1 = create_train_test_data_subset(X1,y1,20000)\n",
    "X_train_scaled1,X_test_scaled1 = apply_scale_processing(MinMaxScaler(),X_train1,X_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79d70c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import  mean_absolute_error, mean_squared_error\n",
    "\n",
    "def optimize_hyperparameters_grid_search (estimator_name, estimator, param_grid,X_train, y_train,X_test,y_test) :\n",
    "# Initialize XGBRegressor\n",
    "\n",
    "    # Setup GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        scoring='neg_mean_squared_error',  # or 'r2', 'neg_mean_absolute_error'\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    # Fit the grid search\n",
    "    print(f\"Starting GridSearchCV {estimator_name} fit ...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Ending GridSearchCV {estimator_name} fit ...\")\n",
    "\n",
    "    # Best parameters and score\n",
    "    print(f\"Best estimator : {estimator_name} {grid_search.best_estimator_}\")\n",
    "    print(f\"Best paramters : {estimator_name} {grid_search.best_params_}\")\n",
    "    print(f\"Best score : {estimator_name} {grid_search.best_params_}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BEST PARAMETERS:\")\n",
    "    print(\"=\"*60)\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"{param:20s}: {value}\")\n",
    "\n",
    "    print(f\"\\nBest CV Score (neg MSE): {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Best CV RMSE: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
    "\n",
    "    # Train best model on full training set and evaluate\n",
    "    print(f\"Starting GridSearchCV {estimator_name} predict ...\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST SET PERFORMANCE:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"MSE:  {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "    # Display top 10 parameter combinations\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    results_df['mean_rmse'] = np.sqrt(-results_df['mean_test_score'])\n",
    "    top_10 = results_df.nsmallest(10, 'mean_rmse')[['params', 'mean_rmse', 'std_test_score']]\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 10 PARAMETER COMBINATIONS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(top_10.to_string(index=False))\n",
    "\n",
    "    # Feature importance plot\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(feature_importance)), feature_importance)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature Index')\n",
    "    plt.title('Feature Importance from Best XGBRegressor Model')\n",
    "    plt.tight_layout()\n",
    "   \n",
    "\n",
    "    # Save the best model\n",
    "    # import joblib\n",
    "    # joblib.dump(best_model, 'best_xgb_model.pkl')\n",
    "    # print(\"Best model saved as 'best_xgb_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e279f",
   "metadata": {},
   "source": [
    "#  # hyper parameters XgbRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ad955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "param_grid1 = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'subsample': [0.5, 0.7, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "    'reg_lambda': [0.1, 1.0, 10.0],\n",
    "    'reg_alpha': [0, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize XGBRegressor\n",
    "xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "optimize_hyperparameters_grid_search(\"XGBRegressor\", xgb,param_grid1,X_train_scaled1, y_train1,X_test_scaled1,y_test1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac975039",
   "metadata": {},
   "source": [
    "#  # hyper parameters DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ab3004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV DecisionTreeRegressor fit ...\n",
      "Fitting 5 folds for each of 1728 candidates, totalling 8640 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error']\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "optimize_hyperparameters_grid_search(\"DecisionTreeRegressor\", dt,param_grid,X_train_scaled1, y_train1,X_test_scaled1,y_test1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "immocv-Hc0iPIjN-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
